{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate MIA Features\n",
    "\n",
    "Based on di.py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import prepare_model\n",
    "from metrics import aggregate_metrics, reference_model_registry\n",
    "import json, os\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "from dataloader import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-410m-deduped\"\n",
    "cache_dir = \"/tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = prepare_model(model_name, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"haritzpuerto/the_pile_arxiv_1k_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pile_train = ds['train'].select(range(100))\n",
    "pile_val = ds['validation'].select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_list = [\"k_min_probs\", \"ppl\", \"zlib_ratio\", \"k_max_probs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.05it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics_train = aggregate_metrics(model, tokenizer, pile_train, metric_list, None, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.85it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics_val = aggregate_metrics(model, tokenizer, pile_val, metric_list, None, batch_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the MIA Classifier\n",
    "\n",
    "Based on the linear_di.py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind, chi2, norm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from selected_features import feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(metrics):\n",
    "    keys = list(metrics.keys())\n",
    "    num_elements = len(metrics[keys[0]])\n",
    "    print (f\"Using {num_elements} elements\")\n",
    "    # select a random subset of val_metrics (50% of ids)\n",
    "    ids_train = np.random.choice(num_elements, num_elements//2, replace=False)\n",
    "    ids_val = np.array([i for i in range(num_elements) if i not in ids_train])\n",
    "    new_metrics_train = {}\n",
    "    new_metrics_val = {}\n",
    "    for key in keys:\n",
    "        new_metrics_train[key] = np.array(metrics[key])[ids_train]\n",
    "        new_metrics_val[key] = np.array(metrics[key])[ids_val]\n",
    "    return new_metrics_train, new_metrics_val\n",
    "\n",
    "def remove_outliers(metrics, remove_frac=0.05, outliers = \"zero\"):\n",
    "    # Sort the array to work with ordered data\n",
    "    sorted_ids = np.argsort(metrics)\n",
    "    \n",
    "    # Calculate the number of elements to remove from each side\n",
    "    total_elements = len(metrics)\n",
    "    elements_to_remove_each_side = int(total_elements * remove_frac / 2) \n",
    "    \n",
    "    # Ensure we're not attempting to remove more elements than are present\n",
    "    if elements_to_remove_each_side * 2 > total_elements:\n",
    "        raise ValueError(\"remove_frac is too large, resulting in no elements left.\")\n",
    "    \n",
    "    # Change the removed metrics to 0.\n",
    "    lowest_ids = sorted_ids[:elements_to_remove_each_side]\n",
    "    highest_ids = sorted_ids[-elements_to_remove_each_side:]\n",
    "    all_ids = np.concatenate((lowest_ids, highest_ids))\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "    \n",
    "    trimmed_metrics = np.copy(metrics)\n",
    "    \n",
    "    if outliers == \"zero\":\n",
    "        trimmed_metrics[all_ids] = 0\n",
    "    elif outliers == \"mean\" or outliers == \"mean+p-value\":\n",
    "        trimmed_metrics[all_ids] = np.mean(trimmed_metrics)\n",
    "    elif outliers == \"clip\":\n",
    "        highest_val_permissible = trimmed_metrics[highest_ids[0]]\n",
    "        lowest_val_permissible = trimmed_metrics[lowest_ids[-1]]\n",
    "        trimmed_metrics[highest_ids] =  highest_val_permissible\n",
    "        trimmed_metrics[lowest_ids] =   lowest_val_permissible\n",
    "    elif outliers == \"randomize\":\n",
    "        #this will randomize the order of metrics\n",
    "        trimmed_metrics = np.delete(trimmed_metrics, all_ids)\n",
    "    else:\n",
    "        assert outliers in [\"keep\", \"p-value\"]\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    return trimmed_metrics\n",
    "\n",
    "def normalize_and_stack(train_metrics, val_metrics, normalize=\"train\"):\n",
    "    '''\n",
    "    excpects an input list of list of metrics\n",
    "    normalize val with corre\n",
    "    '''\n",
    "    new_train_metrics = []\n",
    "    new_val_metrics = []\n",
    "    for (tm, vm) in zip(train_metrics, val_metrics):\n",
    "        if normalize == \"combined\":\n",
    "            combined_m = np.concatenate((tm, vm))\n",
    "            mean_tm = np.mean(combined_m)\n",
    "            std_tm = np.std(combined_m)\n",
    "        else:\n",
    "            mean_tm = np.mean(tm)\n",
    "            std_tm = np.std(tm)\n",
    "        \n",
    "        if normalize == \"no\":\n",
    "            normalized_vm = vm\n",
    "            normalized_tm = tm\n",
    "        else:\n",
    "            #normalization should be done with respect to the train set statistics\n",
    "            normalized_vm = (vm - mean_tm) / std_tm\n",
    "            normalized_tm = (tm - mean_tm) / std_tm\n",
    "        \n",
    "        new_train_metrics.append(normalized_tm)\n",
    "        new_val_metrics.append(normalized_vm)\n",
    "\n",
    "    train_metrics = np.stack(new_train_metrics, axis=1)\n",
    "    val_metrics = np.stack(new_val_metrics, axis=1)\n",
    "    return train_metrics, val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "\n",
    "outliers = \"clip\" #  choices=[\"randomize\", \"keep\", \"zero\", \"mean\", \"clip\", \"mean+p-value\", \"p-value\"]\n",
    "\n",
    "keys = list(metrics_train.keys())\n",
    "train_metrics = []\n",
    "val_metrics = []\n",
    "for key in keys:\n",
    "    metrics_train_key = np.array(metrics_train[key])\n",
    "    metrics_val_key = np.array(metrics_val[key])\n",
    "\n",
    "    # remove the top 2.5% and bottom 2.5% of the data\n",
    "    \n",
    "    metrics_train_key = remove_outliers(metrics_train_key, remove_frac = 0.05, outliers = outliers)\n",
    "    metrics_val_key = remove_outliers(metrics_val_key, remove_frac = 0.05, outliers = outliers)\n",
    "\n",
    "    train_metrics.append(metrics_train_key)\n",
    "    val_metrics.append(metrics_val_key)\n",
    "\n",
    "# concatenate the train and val metrics by stacking them\n",
    "\n",
    "# train_metrics, val_metrics = new_train_metrics, new_val_metrics\n",
    "train_metrics, val_metrics = normalize_and_stack(train_metrics, val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 16)\n",
      "(100, 16)\n"
     ]
    }
   ],
   "source": [
    "print(train_metrics.shape)\n",
    "print(val_metrics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux function\n",
    "def get_dataset_splits(_train_metrics, _val_metrics, num_samples):\n",
    "    # get the train and val sets\n",
    "    for_train_train_metrics = _train_metrics[:num_samples]\n",
    "    for_train_val_metrics = _val_metrics[:num_samples]\n",
    "    for_val_train_metrics = _train_metrics[num_samples:]\n",
    "    for_val_val_metrics = _val_metrics[num_samples:]\n",
    "\n",
    "\n",
    "    # create the train and val sets\n",
    "    train_x = np.concatenate((for_train_train_metrics, for_train_val_metrics), axis=0)\n",
    "    train_y = np.concatenate((-1*np.zeros(for_train_train_metrics.shape[0]), np.ones(for_train_val_metrics.shape[0])))\n",
    "    val_x = np.concatenate((for_val_train_metrics, for_val_val_metrics), axis=0)\n",
    "    val_y = np.concatenate((-1*np.zeros(for_val_train_metrics.shape[0]), np.ones(for_val_val_metrics.shape[0])))\n",
    "    \n",
    "    # return tensors\n",
    "    train_x = torch.tensor(train_x, dtype=torch.float32)\n",
    "    train_y = torch.tensor(train_y, dtype=torch.float32)\n",
    "    val_x = torch.tensor(val_x, dtype=torch.float32)\n",
    "    val_y = torch.tensor(val_y, dtype=torch.float32)\n",
    "    \n",
    "    return (train_x, train_y), (val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux functions about MIA classifier\n",
    "\n",
    "def train_model(inputs, y, num_epochs=10000):\n",
    "    num_features = inputs.shape[1]\n",
    "    model = get_model(num_features)\n",
    "        \n",
    "    criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy Loss for binary classification\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Convert y to float tensor for BCEWithLogitsLoss\n",
    "    y_float = y.float()\n",
    "\n",
    "    with tqdm(range(num_epochs)) as pbar:\n",
    "        for epoch in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()  # Squeeze the output to remove singleton dimension\n",
    "            loss = criterion(outputs, y_float)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.set_description('loss {}'.format(loss.item()))\n",
    "    return model\n",
    "\n",
    "def get_model(num_features, linear = True):\n",
    "    if linear:\n",
    "        model = nn.Linear(num_features, 1)\n",
    "    else:\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(num_features, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1)  # Single output neuron\n",
    "        )\n",
    "    return model\n",
    "\n",
    "def get_predictions(model, val, y):\n",
    "    with torch.no_grad():\n",
    "        preds = model(val).detach().squeeze()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    loss = criterion(preds, y.float())\n",
    "    return preds.numpy(), loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux functions about p-values\n",
    "p_sample_list = [2, 5, 10, 20, 50, 100, 150, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "\n",
    "def get_p_value_list(heldout_train, heldout_val):\n",
    "    p_value_list = []\n",
    "    for num_samples in p_sample_list:\n",
    "        heldout_train_curr = heldout_train[:num_samples]\n",
    "        heldout_val_curr = heldout_val[:num_samples]\n",
    "        t, p_value = ttest_ind(heldout_train_curr, heldout_val_curr, alternative='less')\n",
    "        p_value_list.append(p_value)\n",
    "    return p_value_list\n",
    "    \n",
    "    \n",
    "\n",
    "def split_train_val(metrics):\n",
    "    keys = list(metrics.keys())\n",
    "    num_elements = len(metrics[keys[0]])\n",
    "    print (f\"Using {num_elements} elements\")\n",
    "    # select a random subset of val_metrics (50% of ids)\n",
    "    ids_train = np.random.choice(num_elements, num_elements//2, replace=False)\n",
    "    ids_val = np.array([i for i in range(num_elements) if i not in ids_train])\n",
    "    new_metrics_train = {}\n",
    "    new_metrics_val = {}\n",
    "    for key in keys:\n",
    "        new_metrics_train[key] = np.array(metrics[key])[ids_train]\n",
    "        new_metrics_val[key] = np.array(metrics[key])[ids_val]\n",
    "    return new_metrics_train, new_metrics_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.6218028664588928: 100%|██████████| 1000/1000 [00:01<00:00, 707.72it/s]\n"
     ]
    }
   ],
   "source": [
    "num_random = 1 # How many random runs to do?\n",
    "num_samples = 50 # How many samples to use for training and validation?\n",
    "for i in range(num_random):\n",
    "    np.random.shuffle(train_metrics)\n",
    "    np.random.shuffle(val_metrics)\n",
    "    \n",
    "    # train a model by creating a train set and a held out set\n",
    "    (train_x, train_y), (val_x, val_y) = get_dataset_splits(train_metrics, val_metrics, num_samples)\n",
    "    \n",
    "    model = train_model(train_x, train_y, num_epochs = 1000)\n",
    "    preds, loss = get_predictions(model, val_x, val_y)\n",
    "    preds_train, loss_train = get_predictions(model, train_x, train_y)\n",
    "    og_train = preds_train[train_y == 0]\n",
    "    og_val = preds_train[train_y == 1]\n",
    "\n",
    "    heldout_train = preds[val_y == 0]\n",
    "    heldout_val = preds[val_y == 1]\n",
    "    # alternate hypothesis: heldout_train < heldout_val\n",
    "    \n",
    "    if outliers == \"p-value\" or outliers == \"mean+p-value\":\n",
    "        heldout_train = remove_outliers(heldout_train, remove_frac = 0.05, outliers = \"randomize\")\n",
    "        heldout_val = remove_outliers(heldout_val, remove_frac = 0.05, outliers = \"randomize\")\n",
    "\n",
    "    p_value_list = get_p_value_list(heldout_train, heldout_val)\n",
    "\n",
    "    # using the model weights, get importance of each feature, and save to csv\n",
    "    weights = model.weight.data.squeeze().tolist() \n",
    "    features = keys\n",
    "    feature_importance = {feature: weight for feature, weight in zip(features, weights)}\n",
    "    df = pd.DataFrame(list(feature_importance.items()), columns=['Feature', 'Importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.6639361 , -0.36645943,  0.5381946 , -2.1395907 , -0.18874298,\n",
       "       -0.7441065 ,  0.26053375,  0.79215133,  0.40245932,  0.80567044,\n",
       "       -1.3478304 , -1.5925108 , -0.6377495 ,  1.7314936 , -0.23978443,\n",
       "       -0.28761864,  1.7737526 , -0.71792346, -1.0783162 ,  1.3108569 ,\n",
       "        1.2593244 ,  1.0556872 ,  0.32088178, -0.41791135,  1.4035882 ,\n",
       "       -0.5711216 ,  0.21770804,  0.69964176, -1.6696621 ,  0.56620973,\n",
       "        1.1084148 , -0.15289994, -1.7719492 ,  1.919026  , -0.14105304,\n",
       "       -0.42632312,  0.35625046,  0.5107425 ,  1.079502  ,  1.1825978 ,\n",
       "        0.5884947 ,  0.7529997 , -0.02442701,  0.3700201 ,  0.17306606,\n",
       "        0.47691858, -1.4506899 ,  1.0371825 , -0.02043207,  0.99005145],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heldout_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ppl</td>\n",
       "      <td>-0.199422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>k_min_probs_0.05</td>\n",
       "      <td>0.103656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>k_min_probs_0.1</td>\n",
       "      <td>-0.416267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>k_min_probs_0.2</td>\n",
       "      <td>2.083430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>k_min_probs_0.3</td>\n",
       "      <td>0.784070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>k_min_probs_0.4</td>\n",
       "      <td>-4.846397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>k_min_probs_0.5</td>\n",
       "      <td>0.682575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>k_min_probs_0.6</td>\n",
       "      <td>2.636874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>k_max_probs_0.05</td>\n",
       "      <td>0.410194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>k_max_probs_0.1</td>\n",
       "      <td>-1.433336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>k_max_probs_0.2</td>\n",
       "      <td>4.629175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>k_max_probs_0.3</td>\n",
       "      <td>0.435057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>k_max_probs_0.4</td>\n",
       "      <td>-1.993551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>k_max_probs_0.5</td>\n",
       "      <td>-1.584858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>k_max_probs_0.6</td>\n",
       "      <td>-1.258586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>zlib_ratio</td>\n",
       "      <td>-0.353033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Feature  Importance\n",
       "0                ppl   -0.199422\n",
       "1   k_min_probs_0.05    0.103656\n",
       "2    k_min_probs_0.1   -0.416267\n",
       "3    k_min_probs_0.2    2.083430\n",
       "4    k_min_probs_0.3    0.784070\n",
       "5    k_min_probs_0.4   -4.846397\n",
       "6    k_min_probs_0.5    0.682575\n",
       "7    k_min_probs_0.6    2.636874\n",
       "8   k_max_probs_0.05    0.410194\n",
       "9    k_max_probs_0.1   -1.433336\n",
       "10   k_max_probs_0.2    4.629175\n",
       "11   k_max_probs_0.3    0.435057\n",
       "12   k_max_probs_0.4   -1.993551\n",
       "13   k_max_probs_0.5   -1.584858\n",
       "14   k_max_probs_0.6   -1.258586\n",
       "15        zlib_ratio   -0.353033"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.298164900660768,\n",
       " 0.5653108109572574,\n",
       " 0.46028305708833517,\n",
       " 0.4521887443828846,\n",
       " 0.19495937087732057,\n",
       " 0.19495937087732057,\n",
       " 0.19495937087732057,\n",
       " 0.19495937087732057,\n",
       " 0.19495937087732057,\n",
       " 0.19495937087732057,\n",
       " 0.19495937087732057,\n",
       " 0.19495937087732057,\n",
       " 0.19495937087732057,\n",
       " 0.19495937087732057,\n",
       " 0.19495937087732057,\n",
       " 0.19495937087732057]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.51663136, -0.1560684 ,  0.5157334 , -0.21615733,  0.52323455,\n",
       "        1.8859843 ,  0.511774  , -0.625938  , -0.99483424, -0.41054124,\n",
       "        1.4793929 ,  0.22003429, -0.9509558 ,  0.13129918, -0.22658832,\n",
       "       -1.0837651 , -0.5716633 , -0.11621578,  0.5561568 , -0.8394647 ,\n",
       "        0.4006781 , -0.09853537,  0.88472337,  0.8128087 ,  0.18891428,\n",
       "       -0.06233807, -0.15602644,  0.5759302 , -1.1861213 ,  0.3780684 ,\n",
       "       -0.85187584,  0.44206327, -1.2670323 ,  1.8527066 ,  0.04044168,\n",
       "       -0.7800655 ,  1.0773345 ,  1.4372088 ,  0.412579  , -0.24890132,\n",
       "       -0.2590168 , -0.1424504 ,  0.50125974,  0.04421322,  1.0535867 ,\n",
       "        0.71610206, -1.673247  , -0.5741055 , -1.6307869 ,  0.26277822],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heldout_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
